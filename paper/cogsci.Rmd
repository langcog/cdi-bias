---
title: "Words aren't created equal: Investigating bias on the CDI"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
    Children's early language skill has been linked to later educational outcomes, making
    it important to accurately measure early language. Parent-reported instruments such as the 
    Communicative Development Inventories (CDIs) have been shown to provide valid, consistent 
    measures of children's aggregate early language skill. However, CDIs are predominantly 
    comprised of hundreds of vocabulary items, some of which may not be heard (and thus learned) 
    equally often by children of varying backgrounds. Here, we use a database of American English 
    CDIs to identify words that show strong bias for particular groups of children, along the axes
    of sex (male vs. female), race/ethnicity (white vs. non-white), and socioeconomic status 
    (high vs. low). For each axis, we identify dozens of strongly biased items, and show that 
    eliminating these items reduces the expected ability difference between groups. For sex, we
    consider how to propose replacement words that may show less bias, on the basis of their 
    relatively equal frequency in adult speech directed to male and female children.
    
keywords: >
    language acquisition; word learning; measuring instrument bias; development;
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)

```

# Introduction 

Researchers, clinicians, and parents have long been fascinated with measuring and theorizing about the growth of young children's vocabulary.
Children's early vocabulary growth is assumed to reflect not only their exposure to child-directed speech, but also the varying difficulty of different types of words, and the aptitude of the child -- including potential language deficits.
For example, @bornstein2016stability found stability in language skills

Children's demographics have also been found to be predictive of their language skills.

introduce importance of measuring early language development (e.g., links to later educational outcomes)

The MacArthur-Bates Communicative Development Inventories [CDIs; @Fenson2007] are a set of parent-reported measures of children's productive and receptive language skills, which produce a low-cost and reliable way to estimate children's early language skills [@fenson1994].
CDIs have shown good predictive validity [e.g., @fenson1994; Bornstein et al. 2012, Duff et al. 2015].

discuss potential for bias

what does between-group bias look like?

IRT - what is a good measure? 

## Fundamental DIF Problem


what is our goal for measuring vocabulary? - identifying language delays, predicting later reading, or talking...

how you select items influences how well you achieve these goals (and what bias you find)

We take the approach put forward by @stenhaug2021treading: GLIMMER plots based on the 1-parameter logistic are sufficient to identify bias, without potentially obscuring DIF in a more complex model's additional parameters.

Differential item functioning (DIF) is a technique in IRT used to identify items that show bias against demographic groups. 
DIF is a statistical characteristic of an item that shows the extent to which the item might operate differently or measure varying abilities for subgroups and members of separate demographic groups. 
DIF is, however, a challenging metric for identifying bias for a couple reasons. 
Firstly, DIF is extremely hard to accurately root out. 
The process of finding DIF involves using the test you used to do IRT analysis in order to look for bias within the very same test. 
Secondly, the presence of DIF does not necessarily indicate the presence of bias; it can indicate that one demographic has a higher average ability level than the other. 
These two issues are the basis upon which we get the Fundamental DIF Identification problem. 

This problem can be be understood through a concrete example that is well-known from research on early language learning: consider the fact that females show a larger vocabulary than males across early development [see @frank2021 Ch. 6?; OTHER REFS]. 
The question is whether or not females actually have a higher ability level, or if the tests predominantly have words that are easier for females (i.e., biased).
(It could even be that the test is biased toward males but the language ability of females is large enough to overcome that bias.)
To address this question, we need to know the ability levels of girls and boys in order to confirm that if a word is learned earlier by girls it is simply because of an ability difference rather than a bias inherent to the word. The problem arises from the fact that we measure ability level using the very same test that we are trying to check for biased words. 
If the boys and girls were of the same average language ability, this would not be an issue but given the evidence showing that girls have a higher ability level, we need to know the difference in ability so that when we find DIF for a specific word can be sure it is outside of the expecting DIF that results naturally from their difference in ability.

The outline of this paper is as follows.
First, we will introduce the Wordbank data and the Rasch model we use to analyze the data.

# Methods

## Vocabulary Data

```{r import-data}
load(here("data/en_ws_production.Rdata")) # d_demo, d_mat, en_wg (items)

#Add this exclusion to the Analysis 
removed <- d_demo %>%
  filter(comprehension!=0)
#this is the actually d_demo we used so we must use this in our report
d_demo <- d_demo %>% 
  #filter(comprehension!=0, !is.na(sex)) %>% # can't fit children not producing words, or with NA sex in group model
  arrange(data_id) %>%
  mutate(eth_group = ifelse(ethnicity=="White", "White", "Nonwhite"),
         ses_group = 
           ifelse(is.element(mom_ed, c("None", "Primary", "Some Secondary", "Secondary")), "low", "high"))
```

### Participants

```{r, echo=F}
sex_tab <- table(d_demo$sex)
ses_tab <- table(d_demo$ses_group)
eth_tab <- table(d_demo$ethnicity)
eth2_tab <- table(d_demo$eth_group)
```


We analyze parent-reported Wordbank data from `r nrow(d_demo)` American English CDI: Words & Sentences administrations for children `r min(d_demo$age)` to `r max(d_demo$age)` months of age [@frank2017; @frank2021].
Unlike other projects Wordbank relies on the kindness of others to contribute their data which means that often meta-data for some sets is missing. 
When they received complete information, the Wordbank data set collected demographic data consisting of Birth Order, Race/Ethnicity, Sex and Mothers Education. 
We focused on comparing data between demographic groups on the axes of Sex, Ethnicity and Mother's Education, a proxy for socioeconomic status (henceforth, SES). 
Our data included `r sex_tab["Female"]` female and `r sex_tab["Male"]` male participants, and an additional `r sum(is.na(d_demo$sex))` participants whose sex remained unreported. 
In terms of Ethnicity data was recorded from `r eth_tab["White"]` white, `r eth_tab["Asian"]` Asian, `r eth_tab["Black"]` Black, `r eth_tab["Hispanic"]` Hispanic, and `r eth_tab["Other"]` “Other” participants as well as `r sum(is.na(d_demo$ethnicity))` unreported. Given our distribution of ethnicities, for our analysis we split participants into White (`r eth2_tab["White"]`) and Non-White (`r eth2_tab["Nonwhite"]`). 
Finally, we split participants into high and low SES, of which we had `r ses_tab["high"]` high SES participants, corresponding to mothers' education of some college or higher and `r ses_tab["low"]` Low SES participants. 
<!-- same for eth, SES -->


## Rasch Model

The Rasch model, also known as the 1-parameter logistic (1PL) model, is the simplest Item Response Theory model, and is thus the easiest to use to investigate potential differences in item function across different groups of participants.
The goal of the Rasch model is to jointly estimate for each child $j$ a latent ability $\theta_j$, and for each item $i$ a difficulty parameter $b_i$.
In the model, the probability of child $j$ knowing (i.e., producing or understanding) a given item $i$ is 

$$P_{i}(x_i = 1 | b_{i},\theta_j ) = \frac{1}{1 + e^{-D(\theta_j - b_i )}}$$

where $D$ is a constant scaling parameter ($D=1.702$) which makes the logistic closely match the ogive function in traditional factor analysis [@R-mirt; @reckase2009].
Child ability ($\theta$) and item difficulty ($b$) distributions are standardized (i.e., mean of 0), and expected to be normally-distributed. 
Children with high latent ability ($\theta$) will be more likely to produce any given item than children with lower latent ability, and more difficult items will be produced by fewer children (at any given $\theta$) than easier items.

For each demographic dimension (sex, socioeconomic status, and ethnicity) we fit a separate Rasch model for each demographic group.


# Results

<!-- put the GLIMMERs side-by-side?
```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}

```
-->

```{r sex-glimmer, out.width="\\linewidth", fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.2, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "GLIMMER plot of a sample of CDI:WS words from the sex bias model. Words at the top are easier to learn for females, while those at the bottom are easier for males."}
#img <- png::readPNG("figs/lab_logo_stanford.png")
#grid::grid.raster(img)
knitr::include_graphics("figs/smGLIMMER_sex_prodWS.pdf")
```


```{r ses-glimmer, out.width="\\linewidth", fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "GLIMMER plot of a sample of CDI:WS words from the SES bias model. Words at the top are easier to learn for children from low-SES families, while those at the bottom are easier for those from high-SES families."}
knitr::include_graphics("figs/smGLIMMER_ses_prodWS.pdf")
```


```{r eth-glimmer, out.width="\\linewidth", fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "GLIMMER plot of a sample of CDI:WS words from the ethnicity bias model. Words at the top are easier to learn for children from low-SES families, while those at the bottom are easier for those from high-SES families."}
knitr::include_graphics("figs/smGLIMMER_eth_prodWS.pdf")
```


# Discussion



# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

