---
title: "Estimating demographic bias on tests of children's early vocabulary"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis (kachergis@stanford.edu)}
    \AND {\large \bf Nathan Francis (nathan99@stanford.edu)}
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, Stanford Unviersity \\ Stanford, CA 94305 USA
    }

abstract: >
    Children's early language skill has been linked to later educational outcomes, making
    it important to accurately measure early language. Parent-reported instruments such as the 
    Communicative Development Inventories (CDIs) have been shown to provide valid, consistent 
    measures of children's aggregate early language skill. However, CDIs are predominantly 
    comprised of hundreds of vocabulary items, some of which may not be heard (and thus learned) 
    equally often by children of varying backgrounds. Here, we use a database of American English 
    CDIs to identify words that show strong bias for particular groups of children, on dimensions
    of sex (male vs. female), race/ethnicity (white vs. non-white), and socioeconomic status 
    (high vs. low). For each dimension, we identify dozens of strongly biased items, and show that 
    eliminating these items reduces the expected ability difference between groups. For sex, we
    consider how to propose replacement words that may show less bias, on the basis of their 
    relatively equal frequency in adult speech directed to male and female children.
    
keywords: >
    language acquisition; word learning; measuring instrument bias; development;
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)

```

# Introduction 

Researchers, clinicians, and parents have long been fascinated with the surprising speed and variability in the growth of young children's vocabulary.
Children's early vocabulary growth is assumed to reflect not only their exposure to child-directed speech, but also the varying difficulty of different types of words, and individual differences in the aptitude of the child -- including potential language deficits.
Research has uncovered both within-child consistency in development, as well as significant influence from external factors.
For example, @bornstein2016stability found stability in core language skills across 10 years of children's development, despite changes in maternal income and education over the study period.
Yet socioeconomic status (SES) has also been found to be predictive of children's early language skill, and of later educational outcomes [for a review, see @schwab2016].
Demographic factors are also predictive of language skill: first-born children tend to outpace their siblings, and female children tend to have better language skills than their age-matched male counterparts [@fenson1994] -- a sex-based verbal advantage that continues through high school [see @petersen2018gender for a review].
However, it is also difficult to measure language skill: in any short recording children are unlikely to use all of the words and constructions that they know, and any comprehensive battery of language measures will likely exhaust children's attention span.

The MacArthur-Bates Communicative Development Inventories [CDIs; @Fenson2007] are a set of parent-reported measures of children's productive and receptive language skills, which produce a low-cost and reliable way to estimate children's early language skills [@fenson1994].
CDIs have shown good predictive validity [e.g., @fenson1994; Bornstein et al. 2012, Duff et al. 2015].
Our focus will be on the vocabulary checklist portion of the CDI Words & Sentences (CDI:WS) form, comprised of 680 early-learned words across 22 categories (e.g., animals, vehicles, action words, pronouns) selected to assess the productive vocabulary of children 16 to 30 months of age.
For each item on the CDI:WS, caregivers are asked to respond whether the target child has been heard to say (i.e. produce) the given item.[^1]
Children's total vocabulary score on the CDI:WS is tightly correlated with other facets of early language (e.g., grammatical competence and gesture), suggesting that the language system is "tightly woven" [@frank2021].
Due to these desirable properties, CDIs have been adapted to dozens of languages, and a central repository of CDI data contributed from all over the world has been created [Wordbank; @frank2017; @frank2021].

[^1]: The CDI: Words and Gestures form includes a subset of the CDI:WS items and targets children 12 to 18 months of age, measuring both comprehension and production.

Inspired by the utility and widespread use of the CDI, researchers have recently been using psychometric models on CDI data to construct short, adaptive tests to reliably assess language ability using only a small subset of the CDI items [e.g., @mayor2019, @kachergis2021cat].
These psychometric models typically come from the Item-Response Theory (IRT) framework [@Baker2001], which assumes that not only test-takers (here, children) have normally-distributed ability, but that items (words on the CDI) have normally-distributed difficulty.
The very efficacy of these IRT-based models depends on words varying in difficulty, and hence being more/less informative of the ability level of different individuals.
For example, asking whether a 22-month-old produces the word "ball" is far less informative of that child's language ability than asking whether they produce "table", as 96% of 22-month-olds can produce the former, while only 47% produce the latter.

While it is quite reasonable to expect that some CDI words are easier than others, and even to use these varying difficulties to predict variation in children's language ability, the use of psychometric models highlights the possibility that some CDI items may function differently (i.e., be more/less difficult) for different groups of children.
The idea that some items on a test may show bias, favoring one group over another, is known as Differential Item Function (DIF; @holland1993differential]. 
On any given test, it is clearly undesirable to have more items favoring one group (say, children from rural households) over another (urban children), as the test will overestimate the ability of test-takers in the former (rural) group -- and not because of any underlying mean difference in ability between the groups, but simply because the test is unfair.
A variety of statistical methods for detecting DIF have been proposed, and investigations have in several instances identified DIF for many items on tests favoring particular one group over another (e.g., rural vs. urban).
Our goal here is to test the items on the American CDI:WS for DIF along three main axes: sex (male vs. female), socioeconomic status (low- vs. high-SES), and ethnicity (white vs. non-white).

However, DIF is fundamentally difficult to identify for multiple reasons [for an overview, see @stenhaug2021treading].
First, most techniques to identify DIF rely on defining a set of "anchor" test items that are assumed to be equally difficult (i.e., unbiased) for both groups of interest.
Identifying anchor items is at best fraught when there may in fact be a difference in ability between groups (e.g., the female language advantage), and is further confounded when the magnitude of this ability difference is unknown.
A reason that DIF is particularly tricky in measuring children's early language ability is that there is a finite universe of early-learned words to choose from--and we may expect many of them to be biased for various environmental reasons (e.g., children in Florida may not use mittens or skis).
Hence, the presence of DIF on a wide variety of items may not indicate the presence of bias; it could indicate that one demographic has a higher average ability level than the other.

<!--
This problem can be be understood through a concrete example that is well-known from research on early language learning: consider the fact that females show a larger vocabulary than males across early development [see @frank2021 Ch. 6?; OTHER REFS]. 
The question is whether or not females actually have a higher ability level, or if the tests predominantly have words that are easier for females (i.e., biased).
(It could even be that the test is biased toward males but the language ability of females is large enough to overcome that bias.)
To address this question, we need to know the ability levels of girls and boys in order to confirm that if a word is learned earlier by girls it is simply because of an ability difference rather than a bias inherent to the word. The problem arises from the fact that we measure ability level using the very same test that we are trying to check for biased words. 
If the boys and girls were of the same average language ability, this would not be an issue but given the evidence showing that girls have a higher ability level, we need to know the difference in ability so that when we find DIF for a specific word can be sure it is outside of the expecting DIF that results naturally from their difference in ability.
-->

The outline of this paper is as follows.
First, we will introduce the Wordbank data and the IRT model we use to analyze the data.
We then fit the IRT model to each group along each dimension, and examine the parameters for evidence of DIF 

# Methods

## Vocabulary Data

```{r import-data}
load(here("data/en_ws_production.Rdata")) # d_demo, d_mat, en_wg (items)

#Add this exclusion to the Analysis 
removed <- d_demo %>%
  filter(comprehension!=0)
#this is the actually d_demo we used so we must use this in our report
d_demo <- d_demo %>% 
  #filter(comprehension!=0, !is.na(sex)) %>% # can't fit children not producing words, or with NA sex in group model
  arrange(data_id) %>%
  mutate(eth_group = ifelse(ethnicity=="White", "White", "Nonwhite"),
         ses_group = 
           ifelse(is.element(mom_ed, c("None", "Primary", "Some Secondary", "Secondary")), "low", "high"))
```

### Participants

```{r, echo=F}
sex_tab <- table(d_demo$sex)
ses_tab <- table(d_demo$ses_group)
eth_tab <- table(d_demo$ethnicity)
eth2_tab <- table(d_demo$eth_group)
```


We analyze parent-reported Wordbank data from `r nrow(d_demo)` American English CDI: Words & Sentences administrations for children `r min(d_demo$age)` to `r max(d_demo$age)` months of age [@frank2017; @frank2021].
Full demographic data are not reported in some datasets contributed to Wordbank: sex was available for `r nrow(d_demo)-sum(is.na(d_demo$sex))` children, race/ethnicity was available for `r nrow(d_demo)-sum(is.na(d_demo$ethnicity))` children, and mother's education (a proxy for socioeconomic status; SES) was available for `r nrow(d_demo)-sum(is.na(d_demo$ses_group))` children.

The analysis of sex-based differences included CDI administrations from `r sex_tab["Female"]` female and `r sex_tab["Male"]` male children.
The analysis of race/ethnicity-based differences included data from `r eth_tab["White"]` white, `r eth_tab["Asian"]` Asian, `r eth_tab["Black"]` Black, `r eth_tab["Hispanic"]` Hispanic, and `r eth_tab["Other"]` "Other" children. 
For this analysis we categorized participants' race as White (`r eth2_tab["White"]`) or Non-White (`r eth2_tab["Nonwhite"]`). 
For the SES-based analysis, we categorized the `r ses_tab["high"]` children whose mothers had at least some college or more as high-SES, and those whose mothers had at most high school (`r ses_tab["low"]` children) as low-SES. 


## Rasch Model

The Rasch model, also known as the 1-parameter logistic (1PL) model, is the simplest Item Response Theory model, and is thus the easiest to use to investigate potential differences in item function across different groups of participants.
The goal of the Rasch model is to jointly estimate for each child $j$ a latent ability $\theta_j$, and for each item $i$ a difficulty parameter $b_i$.
In the model, the probability of child $j$ knowing (i.e., producing or understanding) a given item $i$ is 

$$P_{i}(x_i = 1 | b_{i},\theta_j ) = \frac{1}{1 + e^{-D(\theta_j - b_i )}}$$

where $D$ is a constant scaling parameter ($D=1.702$) which makes the logistic closely match the ogive function in traditional factor analysis [@R-mirt; @reckase2009].
Child ability ($\theta$) and item difficulty ($b$) distributions are standardized (i.e., mean of 0), and expected to be normally-distributed. 
Children with high latent ability ($\theta$) will be more likely to produce any given item than children with lower latent ability, and more difficult items will be produced by fewer children (at any given $\theta$) than easier items.

In the multigroup Rasch model, an item's difficulty is allowed to vary depending on which group an individual child belongs to. 
For example, in the sex-based multigroup model, item $i$'s difficulty is $b_{i}^{female}$ for females, and $b_{i}^{male}$ for males.
We fitted a multigroup Rasch model for each demographic dimension (sex, socioeconomic status, and ethnicity).


# Results

<!-- put the GLIMMERs side-by-side?
```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}

```
-->

```{r sex-glimmer, out.width="\\linewidth", fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.2, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "GLIMMER plot of a sample of CDI:WS words from the sex bias model. Words at the top are easier to learn for females, while those at the bottom are easier for males."}
#img <- png::readPNG("figs/lab_logo_stanford.png")
#grid::grid.raster(img)
knitr::include_graphics("figs/smGLIMMER_sex_prodWS.pdf")
```


```{r ses-glimmer, out.width="\\linewidth", fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "GLIMMER plot of a sample of CDI:WS words from the SES bias model. Words at the top are easier to learn for children from low-SES families, while those at the bottom are easier for those from high-SES families."}
knitr::include_graphics("figs/smGLIMMER_ses_prodWS.pdf")
```


```{r eth-glimmer, out.width="\\linewidth", fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "GLIMMER plot of a sample of CDI:WS words from the ethnicity bias model. Words at the top are easier to learn for children from low-SES families, while those at the bottom are easier for those from high-SES families."}
knitr::include_graphics("figs/smGLIMMER_eth_prodWS.pdf")
```


# Discussion



# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

